{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 謝舜卿\n",
    "\n",
    "Student ID: NCCU_113152012\n",
    "\n",
    "GitHub ID: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Phase Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Reddit Stock Sentiment Dataset Analysis\n",
    "print(\"=== Phase 1: Reddit Stock Sentiment Dataset Analysis ===\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the Reddit stock sentiment dataset\n",
    "print(\"Loading Reddit stock sentiment dataset...\")\n",
    "df = pd.read_csv('newdataset/Reddit-stock-sentiment.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"\\n=== Dataset Overview ===\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Text column type: {type(df['text'].iloc[0])}\")\n",
    "print(f\"Label distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n=== Missing Values ===\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Create simplified dataset (like the Master file)\n",
    "print(f\"\\n=== Creating Simplified Dataset ===\")\n",
    "# Keep only necessary columns\n",
    "X = df[['text', 'label']].copy()\n",
    "X = X.dropna()  # Remove rows with missing text or label\n",
    "\n",
    "# Rename columns to match Master file structure\n",
    "X.columns = ['text', 'category']\n",
    "X['category_name'] = X['category'].map({1.0: 'positive', 0.0: 'neutral', -1.0: 'negative'})\n",
    "\n",
    "print(f\"Simplified dataset shape: {X.shape}\")\n",
    "print(f\"Category distribution:\")\n",
    "print(X['category_name'].value_counts())\n",
    "\n",
    "# Sample the data for analysis (like X_sample in Master file)\n",
    "X_sample = X.sample(n=min(1000, len(X)), random_state=42)\n",
    "print(f\"Sample dataset shape: {X_sample.shape}\")\n",
    "\n",
    "# Basic text analysis\n",
    "print(f\"\\n=== Text Analysis ===\")\n",
    "X['text_length'] = X['text'].str.len()\n",
    "print(f\"Text length statistics:\")\n",
    "print(X['text_length'].describe())\n",
    "\n",
    "# Category distribution visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "X['category_name'].value_counts().plot(kind='bar', color=['green', 'orange', 'red'])\n",
    "plt.title('Sentiment Distribution in Reddit Stock Dataset')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length by category\n",
    "plt.figure(figsize=(10, 6))\n",
    "X.boxplot(column='text_length', by='category_name', ax=plt.gca())\n",
    "plt.title('Text Length Distribution by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Text Length (characters)')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Phase 1 basic analysis completed!\")\n",
    "print(\"✅ Dataset loaded and preprocessed successfully!\")\n",
    "print(\"✅ Ready for further analysis similar to Master file exercises!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Print first 3 samples\n",
    "print(\"=== Exercise 1: First 3 Samples ===\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Text: {X.iloc[i]['text'][:200]}...\")\n",
    "    print(f\"Category: {X.iloc[i]['category_name']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Exercise 2: Pandas querying techniques\n",
    "print(\"\\n=== Exercise 2: Pandas Querying ===\")\n",
    "# Basic queries\n",
    "print(\"First 5 rows:\")\n",
    "print(X[['text', 'category_name']].head())\n",
    "\n",
    "# Conditional queries\n",
    "positive_docs = X[X['category_name'] == 'positive']\n",
    "print(f\"\\nPositive documents: {len(positive_docs)}\")\n",
    "\n",
    "# Text length analysis\n",
    "long_docs = X[X['text_length'] > 500]\n",
    "print(f\"Documents longer than 500 characters: {len(long_docs)}\")\n",
    "\n",
    "# Exercise 3: Query every 10th record from positive category\n",
    "print(\"\\n=== Exercise 3: Every 10th Positive Record ===\")\n",
    "positive_records = X[X['category_name'] == 'positive']\n",
    "every_10th = positive_records.iloc[::10]\n",
    "print(f\"Every 10th positive record count: {len(every_10th)}\")\n",
    "print(\"First 3 every 10th records:\")\n",
    "for i in range(min(3, len(every_10th))):\n",
    "    print(f\"Record {i+1}: {every_10th.iloc[i]['text'][:100]}...\")\n",
    "\n",
    "# Exercise 4: Missing values by record\n",
    "print(\"\\n=== Exercise 4: Missing Values by Record ===\")\n",
    "missing_by_record = X.isnull().sum(axis=1)\n",
    "print(f\"Records with missing values: {missing_by_record.sum()}\")\n",
    "print(f\"Records with 0 missing values: {(missing_by_record == 0).sum()}\")\n",
    "\n",
    "# Exercise 5: Missing values analysis\n",
    "print(\"\\n=== Exercise 5: Missing Values Analysis ===\")\n",
    "print(\"Missing values per column:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Exercise 6: Compare X and X_sample\n",
    "print(\"\\n=== Exercise 6: X vs X_sample Comparison ===\")\n",
    "print(f\"Original X shape: {X.shape}\")\n",
    "print(f\"X_sample shape: {X_sample.shape}\")\n",
    "print(f\"Reduction: {((X.shape[0] - X_sample.shape[0]) / X.shape[0]) * 100:.1f}%\")\n",
    "\n",
    "# Category distribution comparison\n",
    "print(\"\\nCategory distribution comparison:\")\n",
    "print(\"Original X:\")\n",
    "print(X['category_name'].value_counts())\n",
    "print(\"\\nX_sample:\")\n",
    "print(X_sample['category_name'].value_counts())\n",
    "\n",
    "print(\"\\n✅ Basic exercises completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing and Visualization\n",
    "print(\"=== Text Processing and Visualization ===\")\n",
    "\n",
    "# Create count vectorizer and fit\n",
    "print(\"Creating term-document matrix...\")\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X['text'])\n",
    "\n",
    "print(f\"Term-document matrix shape: {X_counts.shape}\")\n",
    "print(f\"Vocabulary size: {len(count_vect.get_feature_names_out())}\")\n",
    "\n",
    "# Exercise 7: Dynamic ylim for plotting\n",
    "print(\"\\n=== Exercise 7: Dynamic ylim Parameter ===\")\n",
    "category_counts = X_sample['category_name'].value_counts()\n",
    "max_count = category_counts.max()\n",
    "\n",
    "# Plot with dynamic ylim\n",
    "plt.figure(figsize=(10, 6))\n",
    "X_sample['category_name'].value_counts().plot(kind='bar',\n",
    "                                               title='Sentiment Distribution (Dynamic ylim)',\n",
    "                                               ylim=[0, max_count + 10],\n",
    "                                               color=['green', 'orange', 'red'],\n",
    "                                               rot=0, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exercise 8: Overlaid bar charts comparison\n",
    "print(\"\\n=== Exercise 8: Distribution Comparison ===\")\n",
    "X_counts_sample = X_sample['category_name'].value_counts()\n",
    "X_counts_original = X['category_name'].value_counts()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original X': X_counts_original,\n",
    "    'X_sample': X_counts_sample\n",
    "}).fillna(0)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_df.plot(kind='bar', title='Sentiment Distribution Comparison',\n",
    "                   color=['lightblue', 'lightcoral'], width=0.8)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.legend(['Original X', 'X_sample'])\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exercise 9: Text analysis with analyzer\n",
    "print(\"\\n=== Exercise 9: Text Analysis with Analyzer ===\")\n",
    "analyze = count_vect.build_analyzer()\n",
    "first_text = X.iloc[0]['text']\n",
    "tokenized = analyze(first_text)\n",
    "\n",
    "print(f\"First record category: {X.iloc[0]['category_name']}\")\n",
    "print(f\"Text length: {len(first_text)} characters\")\n",
    "print(f\"Tokenized tokens: {len(tokenized)}\")\n",
    "print(f\"First 10 tokens: {tokenized[:10]}\")\n",
    "\n",
    "# Exercise 10: Find words in term-document matrix\n",
    "print(\"\\n=== Exercise 10: Word Analysis ===\")\n",
    "fifth_record = X_counts[4].toarray()[0]\n",
    "ones_positions = np.where(fifth_record == 1)[0]\n",
    "vocabulary = count_vect.get_feature_names_out()\n",
    "\n",
    "if len(ones_positions) >= 2:\n",
    "    second_word = vocabulary[ones_positions[1]]\n",
    "    print(f\"Second word in fifth record: '{second_word}'\")\n",
    "    print(f\"Total words in fifth record: {len(ones_positions)}\")\n",
    "\n",
    "# Exercise 11: Efficient visualization of sparse matrix\n",
    "print(\"\\n=== Exercise 11: Sparse Matrix Visualization ===\")\n",
    "# Sample for visualization\n",
    "n_docs_sample = min(30, X_counts.shape[0])\n",
    "n_terms_sample = min(50, X_counts.shape[1])\n",
    "\n",
    "doc_indices = np.random.choice(X_counts.shape[0], n_docs_sample, replace=False)\n",
    "term_indices = np.random.choice(X_counts.shape[1], n_terms_sample, replace=False)\n",
    "\n",
    "sample_matrix = X_counts[doc_indices][:, term_indices].toarray()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(sample_matrix, cmap='Reds', aspect='auto')\n",
    "plt.colorbar(label='Frequency')\n",
    "plt.title('Sparse Matrix Visualization (Sample)')\n",
    "plt.xlabel('Terms (sampled)')\n",
    "plt.ylabel('Documents (sampled)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exercise 12: Interactive visualization with plotly\n",
    "print(\"\\n=== Exercise 12: Interactive Visualization ===\")\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create interactive heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=sample_matrix,\n",
    "    colorscale='Reds',\n",
    "    hoverongaps=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Interactive Sparse Matrix Visualization',\n",
    "    xaxis_title='Terms',\n",
    "    yaxis_title='Documents',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Exercise 13: Efficient term reduction\n",
    "print(\"\\n=== Exercise 13: Efficient Term Reduction ===\")\n",
    "term_frequencies = np.array(X_counts.sum(axis=0)).flatten()\n",
    "vocabulary = count_vect.get_feature_names_out()\n",
    "\n",
    "# Get top 50 most frequent terms\n",
    "top_n = 50\n",
    "top_indices = np.argsort(term_frequencies)[-top_n:]\n",
    "top_terms = vocabulary[top_indices]\n",
    "top_frequencies = term_frequencies[top_indices]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(range(len(top_terms)), top_frequencies, color='lightblue')\n",
    "plt.title(f'Top {top_n} Most Frequent Terms (Efficient Visualization)')\n",
    "plt.xlabel('Terms')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(range(len(top_terms)), top_terms, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Reduced from {X_counts.shape[1]} terms to {top_n} terms\")\n",
    "print(f\"Computation time reduced by {(1 - top_n/X_counts.shape[1])*100:.1f}%\")\n",
    "\n",
    "# Exercise 14: Long tail pattern\n",
    "print(\"\\n=== Exercise 14: Long Tail Pattern ===\")\n",
    "# Sort terms by frequency\n",
    "sorted_indices = np.argsort(term_frequencies)[::-1]\n",
    "sorted_terms = vocabulary[sorted_indices]\n",
    "sorted_frequencies = term_frequencies[sorted_indices]\n",
    "\n",
    "# Show top 100 terms\n",
    "top_100_terms = sorted_terms[:100]\n",
    "top_100_frequencies = sorted_frequencies[:100]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.fill_between(range(len(top_100_frequencies)), top_100_frequencies, alpha=0.7, color='lightgreen')\n",
    "plt.title('Long Tail Pattern: Frequency Distribution of Terms')\n",
    "plt.xlabel('Term Rank (sorted by frequency)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exercise 15: Log transformation comparison\n",
    "print(\"\\n=== Exercise 15: Log Transformation Comparison ===\")\n",
    "import math\n",
    "\n",
    "# Create log-transformed frequencies\n",
    "term_frequencies_log = [math.log(i) for i in term_frequencies]\n",
    "sorted_frequencies_log = [term_frequencies_log[i] for i in sorted_indices]\n",
    "\n",
    "# Show top 100 for comparison\n",
    "top_100_frequencies_log = sorted_frequencies_log[:100]\n",
    "\n",
    "# Create side-by-side comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Regular frequencies\n",
    "ax1.fill_between(range(len(top_100_frequencies)), top_100_frequencies, alpha=0.7, color='lightblue')\n",
    "ax1.set_title('Regular Term Frequencies')\n",
    "ax1.set_xlabel('Term Rank')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# Log-transformed frequencies\n",
    "ax2.fill_between(range(len(top_100_frequencies_log)), top_100_frequencies_log, alpha=0.7, color='lightgreen')\n",
    "ax2.set_title('Log-Transformed Term Frequencies')\n",
    "ax2.set_xlabel('Term Rank')\n",
    "ax2.set_ylabel('Log(Frequency)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ All Phase 1 exercises completed!\")\n",
    "print(\"✅ Reddit dataset analysis finished!\")\n",
    "print(\"✅ Ready for Phase 2!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Meaningful Visualizations\n",
    "print(\"=== Additional Meaningful Visualizations ===\")\n",
    "\n",
    "# 1. Sentiment distribution over time (if datetime available)\n",
    "print(\"1. Sentiment Analysis Over Time\")\n",
    "if 'datetime' in df.columns:\n",
    "    # Convert datetime and extract date\n",
    "    df['date'] = pd.to_datetime(df['datetime']).dt.date\n",
    "    daily_sentiment = df.groupby(['date', 'label']).size().unstack(fill_value=0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    daily_sentiment.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'])\n",
    "    plt.title('Daily Sentiment Distribution Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Posts')\n",
    "    plt.legend(['Negative', 'Neutral', 'Positive'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Word cloud for each sentiment\n",
    "print(\"2. Word Cloud Analysis\")\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create word clouds for each sentiment\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "    sentiment_text = ' '.join(X[X['category_name'] == sentiment]['text'])\n",
    "    \n",
    "    if sentiment_text:  # Only create wordcloud if text exists\n",
    "        wordcloud = WordCloud(width=400, height=300, \n",
    "                            background_color='white',\n",
    "                            colormap='Reds' if sentiment == 'negative' else \n",
    "                                   'Oranges' if sentiment == 'neutral' else 'Greens').generate(sentiment_text)\n",
    "        \n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(f'{sentiment.capitalize()} Sentiment Word Cloud')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Text length distribution by sentiment with violin plot\n",
    "print(\"3. Text Length Distribution Analysis\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(data=X, x='category_name', y='text_length', palette=['red', 'orange', 'green'])\n",
    "plt.title('Text Length Distribution by Sentiment (Violin Plot)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Text Length (characters)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Most frequent words by sentiment\n",
    "print(\"4. Most Frequent Words by Sentiment\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "    sentiment_docs = X[X['category_name'] == sentiment]['text']\n",
    "    \n",
    "    if len(sentiment_docs) > 0:\n",
    "        # Create count vectorizer for this sentiment\n",
    "        sentiment_vect = CountVectorizer(max_features=20, stop_words='english')\n",
    "        sentiment_counts = sentiment_vect.fit_transform(sentiment_docs)\n",
    "        \n",
    "        # Get word frequencies\n",
    "        word_freq = np.array(sentiment_counts.sum(axis=0)).flatten()\n",
    "        words = sentiment_vect.get_feature_names_out()\n",
    "        \n",
    "        # Sort by frequency\n",
    "        sorted_indices = np.argsort(word_freq)[::-1]\n",
    "        top_words = words[sorted_indices]\n",
    "        top_freq = word_freq[sorted_indices]\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].bar(range(len(top_words)), top_freq, color=['red', 'orange', 'green'][i])\n",
    "        axes[i].set_title(f'Top Words - {sentiment.capitalize()} Sentiment')\n",
    "        axes[i].set_xlabel('Words')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].set_xticks(range(len(top_words)))\n",
    "        axes[i].set_xticklabels(top_words, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Sentiment correlation with text features\n",
    "print(\"5. Sentiment Correlation Analysis\")\n",
    "# Create additional text features\n",
    "X['word_count'] = X['text'].str.split().str.len()\n",
    "X['sentence_count'] = X['text'].str.count(r'[.!?]+')\n",
    "X['avg_word_length'] = X['text'].str.split().str.join(' ').str.len() / X['word_count']\n",
    "\n",
    "# Create correlation matrix\n",
    "numeric_features = ['text_length', 'word_count', 'sentence_count', 'avg_word_length', 'category']\n",
    "correlation_matrix = X[numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix: Text Features vs Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Sentiment distribution by subreddit (if available)\n",
    "if 'subreddit' in df.columns:\n",
    "    print(\"6. Sentiment Distribution by Subreddit\")\n",
    "    subreddit_sentiment = pd.crosstab(df['subreddit'], df['label'])\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    subreddit_sentiment.plot(kind='bar', stacked=True, \n",
    "                            color=['red', 'orange', 'green'])\n",
    "    plt.title('Sentiment Distribution by Subreddit')\n",
    "    plt.xlabel('Subreddit')\n",
    "    plt.ylabel('Number of Posts')\n",
    "    plt.legend(['Negative', 'Neutral', 'Positive'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ Additional meaningful visualizations completed!\")\n",
    "print(\"✅ Phase 1 fully completed with comprehensive analysis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) that considered as **phase 1 (from exercise 1 to exercise 15)**. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** up **until phase 1**. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    -  Use [the new dataset](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/newdataset/Reddit-stock-sentiment.csv). The dataset contains a 16 columns including 'text' and 'label', with the sentiment labels being: 1.0 is positive, 0.0 is neutral and -1.0 is negative. You can simplify the dataset and use only the columns that you think are necessary. \n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "    - Use this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 10% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    \n",
    "\n",
    "\n",
    "4. Fourth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (September 28th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Phase Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can keep the answer for phase 1 for easier running and update the phase 2 on the same page.**\n",
    "\n",
    "1. First: Continue doing the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) for **phase 2, starting from Finding frequent patterns**. Use the same master(.ipynb) file. Answer from phase 1 will not be considered at this stage. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: Continue from first phase and do the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** for phase 2, starting from Finding frequent pattern. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    - Continue using this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output. Use the same new dataset as in phase 1.\n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 20% of your grade.__\n",
    "    - Use this file to answer.\n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency).  Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences and when using augmentation with feature pattern.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 19th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dm2025lab)",
   "language": "python",
   "name": "dm2025lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
